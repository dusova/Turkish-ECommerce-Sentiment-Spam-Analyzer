# ğŸ“š API DokÃ¼mantasyonu / API Documentation

<div align="center">

**[ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e](#tÃ¼rkÃ§e-dokÃ¼mantasyon) | [ğŸ‡¬ğŸ‡§ English](#english-documentation)**

</div>

---

# TÃ¼rkÃ§e DokÃ¼mantasyon

Bu dokÃ¼mantasyon, TÃ¼rkÃ§e E-Ticaret Yorum Analizi projesinin tÃ¼m modÃ¼llerini ve fonksiyonlarÄ±nÄ± detaylÄ± ÅŸekilde aÃ§Ä±klar.

## Ä°Ã§indekiler

1. [src.preprocessing](#srcpreprocessing)
2. [src.model](#srcmodel)
3. [src.spam_detector](#srcspam_detector)
4. [src.utils](#srcutils)
5. [src.app](#srcapp)
6. [config](#config)

---

## src.preprocessing

Metin normalizasyonu ve veri temizleme iÅŸlemlerini iÃ§eren modÃ¼l.

### Fonksiyonlar

#### `turkce_metin_normalize_et(metin, stemming_uygula=True)`

TÃ¼rkÃ§e metni makine Ã¶ÄŸrenmesi iÃ§in uygun hale getirir.

**Parametreler:**
| Parametre | Tip | VarsayÄ±lan | AÃ§Ä±klama |
|-----------|-----|------------|----------|
| `metin` | str | - | Ham metin |
| `stemming_uygula` | bool | True | Stemming uygulanacak mÄ±? |

**DÃ¶nÃ¼ÅŸ:** `str` - Normalize edilmiÅŸ metin

**Uygulanan Ä°ÅŸlemler:**
1. Unicode normalizasyonu (NFKC)
2. KÃ¼Ã§Ã¼k harfe Ã§evirme
3. URL â†’ `<url>` etiketi
4. E-posta â†’ `<email>` etiketi
5. Telefon â†’ `<phone>` etiketi
6. Tekrar eden karakterleri azaltma
7. TurkishStemmer ile kÃ¶k bulma

**Ã–rnek:**
```python
from src.preprocessing import turkce_metin_normalize_et

ham = "HARIKA BÄ°R ÃœRÃœN!!! www.site.com ğŸ˜ğŸ˜ğŸ˜"
temiz = turkce_metin_normalize_et(ham)
print(temiz)  # "harik bir Ã¼rÃ¼n <url>"
```

---

#### `akilli_csv_oku(dosya_yolu)`

CSV dosyasÄ±nÄ± akÄ±llÄ± ÅŸekilde okur (encoding ve ayÄ±rÄ±cÄ± otomatik tespit).

**Parametreler:**
| Parametre | Tip | AÃ§Ä±klama |
|-----------|-----|----------|
| `dosya_yolu` | str | CSV dosyasÄ±nÄ±n yolu |

**DÃ¶nÃ¼ÅŸ:** `pd.DataFrame`

**Desteklenen Encodingler:**
- UTF-8-sig
- UTF-8
- Latin1
- Windows-1254

**Ã–rnek:**
```python
from src.preprocessing import akilli_csv_oku

veri = akilli_csv_oku("data/yorumlar.csv")
print(veri.shape)  # (60000, 3)
```

---

#### `metin_sutunu_bul(veri_cercevesi)`

DataFrame'de metin sÃ¼tununu otomatik tespit eder.

**Parametreler:**
| Parametre | Tip | AÃ§Ä±klama |
|-----------|-----|----------|
| `veri_cercevesi` | pd.DataFrame | Pandas DataFrame |

**DÃ¶nÃ¼ÅŸ:** `str | None` - Bulunan sÃ¼tun adÄ±

---

#### `etiket_sutunu_bul(veri_cercevesi)`

DataFrame'de duygu etiketi sÃ¼tununu otomatik tespit eder.

**Parametreler:**
| Parametre | Tip | AÃ§Ä±klama |
|-----------|-----|----------|
| `veri_cercevesi` | pd.DataFrame | Pandas DataFrame |

**DÃ¶nÃ¼ÅŸ:** `Tuple[str | None, str | None]` - (sÃ¼tun_adÄ±, tespit_yÃ¶ntemi)

---

#### `veri_hazirla(dosya_yolu=None)`

Veri setini indirir/okur ve Ã¶n iÅŸleme uygular.

**Parametreler:**
| Parametre | Tip | VarsayÄ±lan | AÃ§Ä±klama |
|-----------|-----|------------|----------|
| `dosya_yolu` | str | None | CSV dosya yolu (None ise otomatik indirir) |

**DÃ¶nÃ¼ÅŸ:** `pd.DataFrame` - HazÄ±rlanmÄ±ÅŸ veri

**Ã‡Ä±ktÄ± DataFrame SÃ¼tunlarÄ±:**
- `ham_metin`: Orijinal yorum
- `metin`: Normalize edilmiÅŸ metin
- `duygu`: Duygu etiketi (0, 1, 2)

---

## src.model

Duygu analizi modelinin eÄŸitimi ve kullanÄ±mÄ±nÄ± iÃ§eren modÃ¼l.

### SÄ±nÄ±flar

#### `SentimentModel`

TF-IDF + Logistic Regression tabanlÄ± duygu analizi modeli.

**Ã–zellikler:**
| Ã–zellik | Tip | AÃ§Ä±klama |
|---------|-----|----------|
| `pipeline` | Pipeline | Sklearn Pipeline |
| `classes` | dict | SÄ±nÄ±f isimleri |
| `is_trained` | bool | Model eÄŸitildi mi? |

**Metodlar:**

##### `__init__(tfidf_config=None)`
Model oluÅŸturur.

```python
from src.model import SentimentModel

model = SentimentModel()
# veya Ã¶zel config ile
model = SentimentModel(tfidf_config={
    "ngram_range": (1, 3),
    "max_features": 5000
})
```

##### `fit(X, y)`
Modeli eÄŸitir.

**Parametreler:**
| Parametre | Tip | AÃ§Ä±klama |
|-----------|-----|----------|
| `X` | List[str] | Metin listesi |
| `y` | List[int] | Etiket listesi (0, 1, 2) |

**DÃ¶nÃ¼ÅŸ:** `SentimentModel` - EÄŸitilmiÅŸ model

```python
model.fit(X_train, y_train)
```

##### `predict(X)`
Tahmin yapar.

**Parametreler:**
| Parametre | Tip | AÃ§Ä±klama |
|-----------|-----|----------|
| `X` | List[str] | Metin listesi |

**DÃ¶nÃ¼ÅŸ:** `np.ndarray` - Tahmin edilen sÄ±nÄ±flar

```python
tahminler = model.predict(["Harika Ã¼rÃ¼n!", "Berbat kalite"])
print(tahminler)  # [2, 0]
```

##### `predict_proba(X)`
OlasÄ±lÄ±k tahmini yapar.

**DÃ¶nÃ¼ÅŸ:** `np.ndarray` - Shape: (n_samples, 3)

```python
olasiliklar = model.predict_proba(["Harika Ã¼rÃ¼n!"])
print(olasiliklar)  # [[0.05, 0.10, 0.85]]
```

##### `evaluate(X, y)`
Model performansÄ±nÄ± deÄŸerlendirir.

**DÃ¶nÃ¼ÅŸ:** `dict` - Metrikler

```python
metrikler = model.evaluate(X_test, y_test)
print(metrikler["accuracy"])  # 0.85
```

##### `save(path)`
Modeli dosyaya kaydeder.

```python
model.save("models/sentiment_model.pkl")
```

##### `load(path)` (classmethod)
Modeli dosyadan yÃ¼kler.

```python
model = SentimentModel.load("models/sentiment_model.pkl")
```

---

## src.spam_detector

Spam/bot yorum tespiti iÅŸlemlerini iÃ§eren modÃ¼l.

### SÄ±nÄ±flar

#### `SpamDetector`

Hibrit spam tespit sistemi (Kural + IsolationForest).

**Ã–zellikler:**
| Ã–zellik | Tip | VarsayÄ±lan | AÃ§Ä±klama |
|---------|-----|------------|----------|
| `rule_weight` | float | 0.6 | Kural aÄŸÄ±rlÄ±ÄŸÄ± |
| `anomaly_weight` | float | 0.4 | Anomali aÄŸÄ±rlÄ±ÄŸÄ± |
| `is_trained` | bool | False | Model eÄŸitildi mi? |

**Metodlar:**

##### `__init__(rule_weight=0.6, anomaly_weight=0.4)`

```python
from src.spam_detector import SpamDetector

detector = SpamDetector()
# veya Ã¶zel aÄŸÄ±rlÄ±klarla
detector = SpamDetector(rule_weight=0.7, anomaly_weight=0.3)
```

##### `kural_tabanli_skor(ham_metin, normalize_metin)`
Kural tabanlÄ± spam skoru hesaplar.

**DÃ¶nÃ¼ÅŸ:** `int` - 1 (spam), 0 (gerÃ§ek), -1 (belirsiz)

**Kontrol Edilen Kurallar:**
- URL/email/telefon varlÄ±ÄŸÄ± (+3 puan)
- Ãœnlem sayÄ±sÄ± >= 4 (+1 puan)
- Emoji sayÄ±sÄ± >= 3 (+1 puan)
- BÃ¼yÃ¼k harf oranÄ± > 0.6 (+1 puan)
- KÄ±sa + jenerik ifade (+2 puan)

##### `fit(ham_metinler, normalize_metinler)`
Spam tespit modelini eÄŸitir.

```python
detector.fit(ham_listesi, normalize_listesi)
```

##### `analyze(ham_metin, normalize_metin=None)`
Tek bir yorumu analiz eder.

**DÃ¶nÃ¼ÅŸ:** `dict`

```python
sonuc = detector.analyze("MUKEMMEL!!! www.site.com")
print(sonuc)
# {
#   "spam_olasiligi": 0.87,
#   "tahmin": 1,
#   "etiket": "Spam",
#   "kural_skoru": 1,
#   "aciklama": "URL tespit edildi, aÅŸÄ±rÄ± Ã¼nlem..."
# }
```

##### `predict(metinler)`
Toplu tahmin yapar.

```python
tahminler = detector.predict(normalize_listesi)
```

##### `save(path)` / `load(path)`
Model kaydetme ve yÃ¼kleme.

---

## src.utils

YardÄ±mcÄ± fonksiyonlarÄ± iÃ§eren modÃ¼l.

### Fonksiyonlar

#### `veri_indir(hedef_yol=None, kaynak="auto")`

Veri setini indirir veya yerel dosyadan okur.

**Parametreler:**
| Parametre | Tip | VarsayÄ±lan | AÃ§Ä±klama |
|-----------|-----|------------|----------|
| `hedef_yol` | str | None | Ä°ndirme hedef yolu |
| `kaynak` | str | "auto" | "huggingface", "github", "local" veya "auto" |

**DÃ¶nÃ¼ÅŸ:** `pd.DataFrame`

**Ã–ncelik SÄ±rasÄ± (auto mod):**
1. Yerel dosya
2. HuggingFace
3. GitHub

```python
from src.utils import veri_indir

veri = veri_indir()  # Otomatik indirir
veri = veri_indir(kaynak="huggingface")  # Sadece HuggingFace
```

#### `model_kaydet(model, yol)`
Modeli pickle formatÄ±nda kaydeder.

#### `model_yukle(yol)`
Modeli pickle formatÄ±ndan yÃ¼kler.

---

## src.app

Gradio web arayÃ¼zÃ¼ modÃ¼lÃ¼.

### Fonksiyonlar

#### `analiz_yap(yorum)`

Bir yorumu tam analiz eder.

**Parametreler:**
| Parametre | Tip | AÃ§Ä±klama |
|-----------|-----|----------|
| `yorum` | str | Yorum metni |

**DÃ¶nÃ¼ÅŸ:** `dict` - Analiz sonuÃ§larÄ±

```python
from src.app import analiz_yap

sonuc = analiz_yap("ÃœrÃ¼n harika, kargo hÄ±zlÄ±ydÄ±!")
print(sonuc)
# {
#   "girdi": "ÃœrÃ¼n harika, kargo hÄ±zlÄ±ydÄ±!",
#   "normalize": "Ã¼rÃ¼n harik kargo hÄ±zl",
#   "spam_analizi": {"olasilik": "12%", "etiket": "GerÃ§ek"},
#   "duygu_analizi": {"genel_duygu": "Pozitif", "olasiliklar": {...}},
#   "aspekt_analizi": {"Kargo": "Pozitif", "Kalite": "Pozitif"}
# }
```

#### `aspekt_analizi(yorum)`
Aspekt bazlÄ± duygu analizi yapar.

### ArayÃ¼zÃ¼ BaÅŸlatma

```python
from src.app import demo

# Yerel sunucu
demo.launch()

# PaylaÅŸÄ±mlÄ± link ile
demo.launch(share=True)

# Belirli port
demo.launch(server_port=7860)
```

---

## config

Merkezi konfigÃ¼rasyon dosyasÄ±.

### Yol Sabitleri

| Sabit | Tip | AÃ§Ä±klama |
|-------|-----|----------|
| `ROOT_DIR` | Path | Proje kÃ¶k dizini |
| `DATA_DIR` | Path | Veri klasÃ¶rÃ¼ |
| `MODELS_DIR` | Path | Model klasÃ¶rÃ¼ |
| `LOGS_DIR` | Path | Log klasÃ¶rÃ¼ |

### Model Parametreleri

| Sabit | DeÄŸer | AÃ§Ä±klama |
|-------|-------|----------|
| `RANDOM_SEED` | 42 | Rastgele tohum |
| `TRAIN_SIZE` | 0.8 | EÄŸitim oranÄ± |
| `SAMPLE_SIZE` | 60000 | Ã–rnekleme boyutu |

### TF-IDF AyarlarÄ±

```python
TFIDF_CONFIG = {
    "ngram_range": (1, 2),
    "min_df": 2,
    "max_df": 0.9,
    "max_features": None
}
```

### Aspekt Anahtar Kelimeleri

```python
ASPECT_KEYWORDS = {
    "kargo_teslimat": ["kargo", "teslimat", "gÃ¶nderim", ...],
    "fiyat_performans": ["fiyat", "ucuz", "pahalÄ±", ...],
    "kalite_malzeme": ["kalite", "malzeme", "saÄŸlam", ...],
    "musteri_hizmetleri": ["iade", "destek", "iletiÅŸim", ...]
}
```

---

# English Documentation

This documentation explains all modules and functions of the Turkish E-Commerce Review Analysis project in detail.

## Table of Contents

1. [src.preprocessing](#srcpreprocessing-1)
2. [src.model](#srcmodel-1)
3. [src.spam_detector](#srcspam_detector-1)
4. [src.utils](#srcutils-1)
5. [src.app](#srcapp-1)
6. [config](#config-1)

---

## src.preprocessing

Module containing text normalization and data cleaning operations.

### Functions

#### `turkce_metin_normalize_et(text, stemming_uygula=True)`

Prepares Turkish text for machine learning.

**Parameters:**
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `text` | str | - | Raw text |
| `stemming_uygula` | bool | True | Apply stemming? |

**Returns:** `str` - Normalized text

**Applied Operations:**
1. Unicode normalization (NFKC)
2. Convert to lowercase
3. URL â†’ `<url>` tag
4. Email â†’ `<email>` tag
5. Phone â†’ `<phone>` tag
6. Reduce repeating characters
7. Turkish stemming with TurkishStemmer

**Example:**
```python
from src.preprocessing import turkce_metin_normalize_et

raw = "HARIKA BÄ°R ÃœRÃœN!!! www.site.com ğŸ˜ğŸ˜ğŸ˜"
clean = turkce_metin_normalize_et(raw)
print(clean)  # "harik bir Ã¼rÃ¼n <url>"
```

---

#### `akilli_csv_oku(file_path)`

Smart CSV reading (auto-detect encoding and delimiter).

**Parameters:**
| Parameter | Type | Description |
|-----------|------|-------------|
| `file_path` | str | Path to CSV file |

**Returns:** `pd.DataFrame`

---

#### `veri_hazirla(file_path=None)`

Downloads/reads dataset and applies preprocessing.

**Returns:** `pd.DataFrame` - Prepared data

**Output DataFrame Columns:**
- `ham_metin`: Original review
- `metin`: Normalized text
- `duygu`: Sentiment label (0, 1, 2)

---

## src.model

Module containing sentiment analysis model training and usage.

### Classes

#### `SentimentModel`

TF-IDF + Logistic Regression based sentiment analysis model.

**Methods:**

##### `fit(X, y)`
Train the model.

##### `predict(X)`
Make predictions.

##### `predict_proba(X)`
Return probabilities.

##### `evaluate(X, y)`
Evaluate model performance.

##### `save(path)` / `load(path)`
Save and load model.

**Example:**
```python
from src.model import SentimentModel

# Train
model = SentimentModel()
model.fit(X_train, y_train)

# Predict
predictions = model.predict(["Great product!", "Terrible quality"])
print(predictions)  # [2, 0]

# Save
model.save("models/sentiment_model.pkl")

# Load
model = SentimentModel.load("models/sentiment_model.pkl")
```

---

## src.spam_detector

Module containing spam/bot detection operations.

### Classes

#### `SpamDetector`

Hybrid spam detection system (Rule-based + IsolationForest).

**Methods:**

##### `fit(raw_texts, normalized_texts)`
Train the spam detection model.

##### `analyze(raw_text, normalized_text=None)`
Analyze a single review.

**Returns:** `dict`
```python
{
  "spam_olasiligi": 0.87,  # Spam probability
  "tahmin": 1,              # Prediction (0=genuine, 1=spam)
  "etiket": "Spam",         # Label
  "kural_skoru": 1,         # Rule-based score
  "aciklama": "..."         # Explanation
}
```

##### `predict(texts)`
Batch prediction.

##### `save(path)` / `load(path)`
Save and load model.

---

## src.utils

Module containing utility functions.

### Functions

#### `veri_indir(target_path=None, source="auto")`

Download dataset or read from local file.

**Parameters:**
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `target_path` | str | None | Download target path |
| `source` | str | "auto" | "huggingface", "github", "local" or "auto" |

---

## src.app

Gradio web interface module.

### Functions

#### `analiz_yap(review)`

Fully analyze a review.

**Returns:** `dict` - Analysis results

### Launch Interface

```python
from src.app import demo

demo.launch(share=True)  # With shareable link
```

---

## config

Central configuration file.

### Path Constants

| Constant | Type | Description |
|----------|------|-------------|
| `ROOT_DIR` | Path | Project root directory |
| `DATA_DIR` | Path | Data folder |
| `MODELS_DIR` | Path | Models folder |

### Model Parameters

| Constant | Value | Description |
|----------|-------|-------------|
| `RANDOM_SEED` | 42 | Random seed |
| `TRAIN_SIZE` | 0.8 | Training ratio |
| `SAMPLE_SIZE` | 60000 | Sample size |

---

<div align="center">

**[ğŸ” BaÅŸa DÃ¶n / Back to Top](#-api-dokÃ¼mantasyonu--api-documentation)**

</div>
